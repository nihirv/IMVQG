{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A tutorial on [Information Maximizing Visual Question Generation](https://arxiv.org/pdf/1903.11207.pdf)\n",
    "Written by [Nihir](mailto:nv419@ic.ac.uk)\n",
    "\n",
    "Resources used:\n",
    "- https://www.borealisai.com/en/blog/tutorial-5-variational-auto-encoders/\n",
    "- https://jaan.io/what-is-variational-autoencoder-vae-tutorial/\n",
    "- https://anotherdatum.com/vae.html\n",
    "- **https://www.youtube.com/watch?v=nug3rz-4Lkg** (Highly recommended)\n",
    "- https://deisenroth.cc/teaching/2018-19/probabilistic-inference/\n",
    "- https://lilianweng.github.io/lil-log/2018/08/12/from-autoencoder-to-beta-vae.html#vae-variational-autoencoder\n",
    "- https://ermongroup.github.io/blog/a-tutorial-on-mmd-variational-autoencoders/#chen2016variational\n",
    "- https://arxiv.org/abs/1906.02691\n",
    "- https://www.cs.cmu.edu/~epxing/Class/10708-17/notes-17/10708-scribe-lecture8.pdf\n",
    "\n",
    "In this tutorial, I attempt to decompose and provide intuition behind the methodology of the titled paper. My background, in general, is not maths based, and my deep learning knowledge coming into this paper did not include variational inference. Because of this, despite being able to follow their code, I was stumped trying to understand the logic of their formulations. Here, I will take us on a journey of reimplementation, starting from the assumption that you have no knowledge about probabilistic modelling, and finishing with an understanding of how, and why:\n",
    "1. Variational Inference works\n",
    "2. Incorporating Mutual Information can enhance the ELBO objective\n",
    "3. The Information Maximising VQG paper works\n",
    "\n",
    "The tutorial will therefore be structured as follows:\n",
    "- Introduction: The case for Bayesian modelling\n",
    " - Definitions\n",
    "- Variational Inference\n",
    " - Deriving ELBO\n",
    "- Incorporating Mutual Information to overcome ELBO shortcomings\n",
    "- Information Maximising Visual Question Generation\n",
    " - Code!\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction: The case for Bayesian modelling\n",
    "\n",
    "In many ML tasks, we want to probabilistically model certain events happening, or probabilistically generate certain data types (e.g. images). Neural networks are tools used in order to compute the representations of the data that we'll use in these models.\n",
    "\n",
    "In the Bayesian modelling framework, we want to learn a probability distribution $P(\\mathbf{x})$ over a multi-dimensional input variable $\\mathbf{x}$. Doing this enables us to generate realistic-looking $x$ samples from the distribution. For example, in the case of images:\n",
    "\n",
    "<img src=\"images/vae_mnist.png\" style=\"width: 800px\" />\n",
    "\n",
    "Other differing algorithms which allow us to do generation exist. GANs are probably the most well known of these. One advantage of probabilstic Bayesian modelling is that we are able to compute uncertainity of our model predictions. A hypothetical stock trading model might output a prediction to \"Buy\", but if it is only 10% sure, this probably isn't a decision I want to take action on:\n",
    "\n",
    "<img src=\"images/uncertainty.png\" style=\"width: 800px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Definitions\n",
    "\n",
    "- **Expectation** $\\mathbb{E}$ of a variable is a probability weighted average\n",
    " - $\\mathbb{E}[X] = \\sum_i x_i p(x_i)$ if $X$ is discrete.\n",
    " - $\\mathbb{E}[X] = \\int_i x_i p(x_i) dx$ if $X$ is continuous.\n",
    "- **Kullback-Leibler divergence** is a non-negative quantity which measures the divergence in two probability distributions\n",
    " - $\\mathcal{D}_{KL}(P || Q) = \\int p(x) \\frac{p(x)}{q(x)} dx$ when $P$ and $Q$ are distributions of a continuous RV \n",
    "- The **product rule** of probability states that a joint probability can be expressed in terms of likelihoods and priors:\n",
    " - $p(A,B) = p(A | B)p(B)$\n",
    "- **Bayes rule** is a way of describes the probability of an event, based on prior knowledge of conditions that might be related to the event:\n",
    " - $p(A | B) = \\frac{p(B|A)p(A)}{p(B)}$\n",
    "- **Mutual information** between two variables tells us how indicative one variable is of another\n",
    " - $I(X, Y) = I(X) + I(Y) - I(XY) /geq 0 $\n",
    " - $I(X, Y) = \\mathcal{D}_{KL}(P{(X,Y)} || P(x) P(y)) = \\mathbb{E}_{P_{(X,Y)}} [\\log \\frac{P(X,Y)}{P(x)P(y)}]$\n",
    " - $I(X, Y) = \\mathbb{H}(Y) - \\mathbb{H}(Y|X)$\n",
    "   - $\\mathbb{H}(Y|X) = \\int_{x, y} p(x, y) \\log \\frac{p(x,y)}{p(x)} dxdy$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Inference\n",
    "\n",
    "Firstly, let's move away from how we currently think about ML problems - that is fitting inputs $\\mathbf{x}$ to a prediction $\\hat{y}$, and optimizing this versus a ground truth $y$: $\\mathcal{L}(\\hat{y}, y)$.\n",
    "\n",
    "We instead want to find a _latent_ variable, $\\mathbf{z} \\in \\mathbb{R}^m$. Each vector in $z$ contains $m$ elements of information that encodes hidden information not directly observable from the training data. If we're generating  MNIST digits for example, abstracting speaking, the first dimension might encode a number representing a digit, the second stroke intensity, the third angle, and so forth.\n",
    "\n",
    "We represent our model using something known as a directed graphical model. This model simply indicate what nodes are reliant on other nodes. For our set up, the graphical model is as follows:\n",
    "\n",
    "<img src=\"images/graph_model.png\" style=\"width: 600px\" />\n",
    "\n",
    "This model tells us that the distribution of $\\mathbf{x}$ is conditioned on $\\mathbf{z}$. Given an image $x$, we want to find at least one latent vector which can \"describe\" this image.\n",
    "\n",
    "Specifically, **we want to calculate the posterior** - we want to infer good values of the latent variable given the observed data. Using Bayes rule, this is given as: $$p(\\mathbf{z | x}) \\triangleq \\frac{p(\\mathbf{x, z})}{p(\\mathbf{x})} \\\\ \n",
    "= \\frac{p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})}{p(\\mathbf{x})}$$\n",
    "\n",
    "- $p(\\mathbf{z|x}$ is the posterior\n",
    "- $p(\\mathbf{x|z})$ is the likelihood\n",
    "- $p(\\mathbf{z})$ is the prior\n",
    "- $p(\\mathbf{x})$ is the evidence\n",
    "\n",
    "As the prior is an assumption we make, and the likelihood is modelled by a multivariate Guassian (we'll talk about this later), we **need to find a way to evaluate $p(\\mathbf{x})$**: $$p(\\mathbf{x}) \\triangleq \\int p(\\mathbf{x, z}) d\\mathbf{z} \\triangleq \\int p(\\mathbf{x|z})p(\\mathbf{z}) d\\mathbf{z}$$ \n",
    "\n",
    "**Where does this come from?**\n",
    "1. The first assignment, $p(\\mathbf{x}) \\triangleq \\int p(\\mathbf{x, z}) d\\mathbf{z}$, is a process known as *marginalisation*.\n",
    " - Marginalisation of a variable, in this case $\\mathbf{x}$, is defined as the probability distribution of $\\mathbf{x}$ when the values of other variable, $\\mathbf{z}$, are not considered.\n",
    " - In the continuous case, this is **calculated by integrating the joint probability distribution** with respect to the other variable: $\\int p(\\mathbf{x, z}) d\\mathbf{z}$\n",
    "2. The second assignment, $\\int p(\\mathbf{x, z}) d\\mathbf{z} \\triangleq \\int p(\\mathbf{x|z})p(\\mathbf{z}) d\\mathbf{z}$ simply comes from Bayes rule: $$p(\\mathbf{x, z}) \\triangleq p(\\mathbf{x}|\\mathbf{z})p(\\mathbf{z})$$\n",
    "\n",
    "**What does it mean?**\n",
    "\n",
    "The integral means that, over the latent space, \n",
    "- for every candidate $z \\in \\mathbf{z}$, what is the likelihood that $x$ was generated from $z$?\n",
    " - For example, if the $z$ encodes information about the digit 7, then a model could be almost as likely to generate an image of a 1 instead of a 7 - because they look similar to each other. It would be highly unlikely for it to produce a 6.\n",
    "- Did we find a good candidate ($z$)? Great! But what's the probability of this $z$ existing?\n",
    " - For example, let's say we were given a image showing an upside down 7. A latent vector, $z$, describing a similar looking 7 with the 'angle dimenson' set to $180^{\\circ}$ would be the ideal $z$ to use to generate a sample. But how likely is this $z$? It isn't likely since digits are not typically drawn upside down.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Ok. So we want to optimize to find ${\\max p(x)}$. Let's use $\\log$ probabilities and see what we're working with: $$\\max \\log p(\\mathbf{x}) = \\max \\log \\int p(\\mathbf{x|z})p(\\mathbf{z}) d\\mathbf{z}$$\n",
    "\n",
    "It turns out that this integral is intractable to compute. Relatively simple expressions for $p(\\mathbf{x|z})$ and $p(\\mathbf{z})$ can describe complex distributions for $p(\\mathbf{x})$. Let's look at an example.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The intractable integral\n",
    "\n",
    "\n",
    "![image.png](images/intractible_example.png)\n",
    "\n",
    "What we want to do is infer nature's process on the creation of these clusters. What processes would nature have to go through create this data in this way?\n",
    "\n",
    "Perhaps nature has access to $K$ clusters, and it picks one of these clusters, $k_i$. Subsequently, it picks from $k_i$'s distribution and returns us a datapoint. By doing this $N$ times, we assume we'll have something similar to the datapoints we've observed.\n",
    "\n",
    "Note that this is just an assumption that we're making about how nature generated these points. The real process could be very different and more complicated, but the aim is that we want to get as close to the real process as possible under these assumptions.\n",
    "\n",
    "**Bayesian mixture of Gaussians**\n",
    "\n",
    "Let's assume that nature samples a cluster $k_i$ from some categorical distribution over $K$, and a point from a Guassian with 0 mean and variance $\\sigma^2$\n",
    "\n",
    "![](images/nature_sampling_process.png)\n",
    "\n",
    "- $\\mu_k \\sim \\mathcal{N}(0, \\sigma^2). k = 1, ..., K$\n",
    " - Cluster centers are sampled from the Gaussian distribution with 0 mean and \\sigma^2 variance.\n",
    " - There are K cluster centers (i.e. one for each cluster)\n",
    "- $\\mathbf{c_i} \\sim \\text{Cat}(K) \\quad \\text{for} \\: i = 1, ..., N$\n",
    " - Each datapoint is part of a distinct cluster\n",
    " - The cluster for this datapoint, $\\mathbf{c_i}$, is a one hot vector\n",
    " - We can initially assume our categorical distribution is uniform\n",
    "\n",
    "What the above conveys is our prior beliefs about something. We can use the cluster center alongside its respective cluster to generate a datapoint: $$\\mathbf{x_i|(c_i, \\mu)} \\sim \\mathcal{N}(\\mathbf{c_i^T\\mu},\\sigma_2^2) \\quad i = 1, ..., N$$\n",
    "\n",
    "Let's break this down a bit...\n",
    "- Each $\\mathbf{c_i^T} \\in \\mathbb{R}^K$ is simply a one hot (row) vector. For example, $[0, 1, ... 0]$\n",
    "- $\\mathbf{\\mu} \\in \\mathbb{R}^K$ is a (column) vector of the cluster centers. For example, $\\mathbf{\\mu} = [\\mu_1, \\mu_2, ..., \\mu_K]^T$\n",
    "- The inner product of one hot vector with another vector essentially returns the value in the second vector at the index where there is a 1 in the one hot vector:\n",
    " - $\\mathbf{c_i^T}\\mu = [0, 1, ..., 0][\\mu_1, \\mu_2, ..., \\mu_K]^T = \\mu_2$\n",
    "\n",
    "This $\\mu_k$ is used as the mean when generating a sample $\\mathbf{x_i}$. We fit the variance $\\sigma_2^2$ during training. If we are able to find the best distribution that the means and variances are sampled from, then we have a good solution to this clustering problem.\n",
    "\n",
    "Here, we'll link this back to our max marginal:\n",
    "$$\\max \\log p(\\mathbf{x}) = \\max \\log \\int p(\\mathbf{x, z}) d\\mathbf{z} = \\max \\log \\int p(\\mathbf{x|z})p(\\mathbf{z}) d\\mathbf{z}$$\n",
    "\n",
    "Our $\\mathbf{z}$ now consists of two vectors, $\\mathbf{\\mu}$ and $\\mathbf{c}$: $$\\mathbf{z} = \\{\\mathbf{\\mu}, \\mathbf{c}\\}$$.\n",
    "\n",
    "Our goal is to maximise $p(\\mathbf{x})$. Forumated by marginalising with the aforementioned $\\mathbf{z}$, we obtain:\n",
    "$$\\log p(\\mathbf{x}) = \\log \\int_\\mu \\sum_c p(\\mathbf{x, c, \\mu}) d\\mathbf{\\mu}$$\n",
    "\n",
    "(We have an integral for $\\mu$ because it is continuous, and a summation for $\\mathbf{c}$ because it is discrete.)\n",
    "\n",
    "We see that our **marginalisation process has as many aggregations (i.e. integrals or summations) as latent dimensions**. This integral has no closed-form solution and thus becomes intractable to solve when we want to be more expressive with our latent variable (i.e. increase the number of dimensions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Variational Inference and ELBO\n",
    "\n",
    "Variational Inference (VI) helps us deal with this kind of problem. **VI transforms computing integerals into an optimization problem**. These problems can then be solved by more familiar optimization techniques (e.g. SGD). We start by introducing a new distribution $q(\\mathbf{z})$. $$\\log \\int p(\\mathbf{x, z}) d\\mathbf{z} = \\log \\int \\frac{q(\\mathbf{z})}{q(\\mathbf{z})}p(\\mathbf{x,z})  d\\mathbf{z} = \\log \\mathbb{E}_{z \\sim q(z)}\\left[\\frac{p(\\mathbf{x, z})}{q(\\mathbf{z})}\\right]$$\n",
    "\n",
    "The first assignment is legal as $\\frac{q(\\mathbf{z})}{q(\\mathbf{z})} = 1$. These $\\mathbf{z}$'s still \"go away\" due to the marginalisation process. The second assignment re-writes this form using the definition of an expectation. Note that we are calculating the expectation w.r.t. $q(\\mathbf{z})$\n",
    "\n",
    "Recall Jensen's inequality for a concave function: $f(\\mathbb{E}[X]) \\geq \\mathbb{E}[f(x)]$.\n",
    "- The LHS computes $f(x)$ over the average of some datapoints, say $x$ and $y$.\n",
    "- The RHS computes $f(x)$ for the datapoints $x$ and $y$, and then averages the result of each $f(x)$\n",
    " \n",
    "<img src=\"images/jensen_eq.png\" style=\"width: 500px;\"/>\n",
    "\n",
    "Since logarithms are concave functions...\n",
    "$$ \\log \\mathbb{E}_{z \\sim q(z)}\\left[\\frac{p(\\mathbf{x, z})}{q(\\mathbf{z})}\\right] \\geq \\mathbb{E}_{z \\sim q(z)} \\left[\\log \\left[\\frac{p(\\mathbf{x, y})}{q(\\mathbf{z})} \\right] \\right] \\\\\n",
    "= \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\log \\left[ \\frac{p(\\mathbf{x|z})p(\\mathbf{z})}{q(\\mathbf{z})} \\right] \\right] \\\\\n",
    "= \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\log p(\\mathbf{x|z}) + \\log \\frac{p(\\mathbf{z})}{q(\\mathbf{z})} \\right] \\\\\n",
    "= \\mathbb{E}_{q(\\mathbf{z})}[\\log p(\\mathbf{x|z})] + \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\frac{p(\\mathbf{z})}{q(\\mathbf{z})} \\right] \\\\\n",
    "= \\mathbb{E}_{q(\\mathbf{z})}[\\log p(\\mathbf{x|z})] - \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\frac{q(\\mathbf{z})}{p(\\mathbf{z})} \\right]\n",
    "$$\n",
    "\n",
    "Notice that $\\mathbb{E}_{q(\\mathbf{z})} \\left[ \\frac{q(\\mathbf{z})}{p(\\mathbf{z})} \\right]$ is the definition of the KL divergence between $q(\\mathbf{z})$ and $p(\\mathbf{z})$. Thus, $$ \\mathbb{E}_{q(\\mathbf{z})}[\\log p(\\mathbf{x|z})] - \\mathbb{E}_{q(\\mathbf{z})} \\left[ \\frac{q(\\mathbf{z})}{p(\\mathbf{z})} \\right] = \\\\ \\mathbb{E}_{q(\\mathbf{z})}[\\log p(\\mathbf{x|z})] - \\mathcal{D}_{KL}\\left(q(\\mathbf{z}) || p(\\mathbf{z})\\right) $$.\n",
    "\n",
    "Because $\\log p(\\mathbf{x}) = \\log \\int p(\\mathbf{x, z}) d\\mathbf{z}$... $$ \\log p(\\mathbf{x}) \\geq \\mathbb{E}_{q(\\mathbf{z})}[\\log p(\\mathbf{x|z})] - \\mathcal{D}_{KL}\\left(q(\\mathbf{z}) || p(\\mathbf{z})\\right)$$\n",
    "\n",
    "This is known as the Evidence Lower-Bound, aka the ***ELBO***. Since $\\mathcal{D}_{KL}(\\cdot||\\cdot)$ is always non-negative, we arrive at the lower bound."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational AutoEncoder\n",
    "\n",
    "The Variational AutoEncoder (VAE) uses the ELBO to solve an auto-encoding task.\n",
    "\n",
    "<img src=\"images/vae.png\" style=\"width: 500px\" />\n",
    "\n",
    "We build an encoder, parameterised by $\\phi$, which approximates the posterior of the true data: $q_{\\phi}(\\mathbf{z|x}) \\approx p_{\\theta}(\\mathbf{z|x})$. $p_{\\theta}$ is a decoder model, parameterized by $\\theta$, which we want to optimize to match the true data distribution. That is, $p_{\\theta^*} = p$.\n",
    "\n",
    "We use a neural network to parameterise both $\\phi$ and $\\theta$: $$(\\mathbf{\\mu}, \\log \\mathbf{\\sigma}) = \\text{EncoderNeuralNetwork}_{\\phi}(\\mathbf{x}) \\\\\n",
    "q_{\\phi}(\\mathbf{z|x}) = \\mathcal{N}\\left(\\mathbf{z}; \\mathbf{\\mu}, \\text{diag}(\\mathbf{\\sigma})\\right) \\\\\n",
    "p_{\\theta}(\\mathbf{x}|\\mathbf{z}) = \\text{DecoderNeuralNetwork}_{\\theta}(\\mathbf{z})\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reparameterization trick\n",
    "Recall our objective: $$ \\mathcal{L}_{\\theta, \\phi}(\\mathbf{x}) = \\mathbb{E}_{q_{\\phi}(\\mathbf{z})}[\\log p_{\\theta}(\\mathbf{x|z})] - \\mathcal{D}_{KL}\\left(q_{\\phi}(\\mathbf{z}) || p(\\mathbf{z})\\right) \\leq  \\log p_{\\theta}(\\mathbf{x})$$\n",
    "\n",
    "The expectation in the first term means we'll need to sample $z \\sim q_{\\phi}(\\mathbf{z} | \\mathbf{x})$. Since sampling is a stochastic process, we cannot backprop the gradient. The make this trainable, [Kingma and Welling](https://arxiv.org/abs/1312.6114) introduced the *reparameterization trick*. This trick enables $\\mathbf{z}$ to be determinsitic by using an auxiliary independent random variable: $$\\mathbf{z} = \\mathbf{\\mu} + (\\mathbf{\\sigma} \\odot \\mathbf{\\epsilon})$$\n",
    "\n",
    "where $\\mathbf{\\epsilon} \\sim \\mathcal{N}(0, \\mathbf{I})$\n",
    "\n",
    "<img src=\"images/reparameterization_trick.png\" style=\"width: 500px\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import matplotlib.pyplot as plt\n",
    "from tqdm import tqdm\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "9920512it [00:03, 2947257.25it/s]                                                                                      \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading http://yann.lecun.com/exdb/mnist/train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "32768it [00:00, 110989.40it/s]                                                                                         \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\train-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "1654784it [00:00, 1770758.59it/s]                                                                                      \n",
      "0it [00:00, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-images-idx3-ubyte.gz to ./data\\MNIST\\raw\n",
      "Downloading http://yann.lecun.com/exdb/mnist/t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "8192it [00:00, 40865.09it/s]                                                                                           \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting ./data\\MNIST\\raw\\t10k-labels-idx1-ubyte.gz to ./data\\MNIST\\raw\n",
      "Processing...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "transforms = transforms.Compose([transforms.ToTensor()])\n",
    "train_dataset = datasets.MNIST(\n",
    "    './data',\n",
    "    train=True,\n",
    "    download=True,\n",
    "    transform=transforms)\n",
    "\n",
    "test_dataset = datasets.MNIST(\n",
    "    './data',\n",
    "    train=False,\n",
    "    download=True,\n",
    "    transform=transforms\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64     # number of data points in each batch\n",
    "N_EPOCHS = 10       # times to run the model on complete data\n",
    "INPUT_DIM = 28 * 28 # size of each input\n",
    "HIDDEN_DIM = 256    # hidden dimension\n",
    "LATENT_DIM = 20     # latent vector dimension\n",
    "lr = 1e-3           # learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_iterator = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
    "test_iterator = DataLoader(test_dataset, batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    ''' This the encoder part of VAE\n",
    "\n",
    "    '''\n",
    "    def __init__(self, input_dim, hidden_dim, z_dim):\n",
    "        '''\n",
    "        Args:\n",
    "            input_dim: A integer indicating the size of input (in case of MNIST 28 * 28).\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            z_dim: A integer indicating the latent dimension.\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(input_dim, hidden_dim)\n",
    "        self.mu = nn.Linear(hidden_dim, z_dim)\n",
    "        self.var = nn.Linear(hidden_dim, z_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, input_dim]\n",
    "\n",
    "        hidden = F.relu(self.linear(x))\n",
    "        # hidden is of shape [batch_size, hidden_dim]\n",
    "        z_mu = self.mu(hidden)\n",
    "        # z_mu is of shape [batch_size, latent_dim]\n",
    "        z_var = self.var(hidden)\n",
    "        # z_var is of shape [batch_size, latent_dim]\n",
    "\n",
    "        return z_mu, z_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    ''' This the decoder part of VAE\n",
    "\n",
    "    '''\n",
    "    def __init__(self, z_dim, hidden_dim, output_dim):\n",
    "        '''\n",
    "        Args:\n",
    "            z_dim: A integer indicating the latent size.\n",
    "            hidden_dim: A integer indicating the size of hidden dimension.\n",
    "            output_dim: A integer indicating the output dimension (in case of MNIST it is 28 * 28)\n",
    "        '''\n",
    "        super().__init__()\n",
    "\n",
    "        self.linear = nn.Linear(z_dim, hidden_dim)\n",
    "        self.out = nn.Linear(hidden_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x is of shape [batch_size, latent_dim]\n",
    "\n",
    "        hidden = F.relu(self.linear(x))\n",
    "        # hidden is of shape [batch_size, hidden_dim]\n",
    "\n",
    "        predicted = torch.sigmoid(self.out(hidden))\n",
    "        # predicted is of shape [batch_size, output_dim]\n",
    "\n",
    "        return predicted\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE(nn.Module):\n",
    "    ''' This the VAE, which takes a encoder and decoder.\n",
    "\n",
    "    '''\n",
    "    def __init__(self, enc, dec):\n",
    "        super().__init__()\n",
    "\n",
    "        self.enc = enc\n",
    "        self.dec = dec\n",
    "\n",
    "    def forward(self, x):\n",
    "        # encode\n",
    "        z_mu, z_var = self.enc(x)\n",
    "\n",
    "        # sample from the distribution having latent parameters z_mu, z_var\n",
    "        # reparameterize\n",
    "        std = torch.exp(z_var / 2)\n",
    "        eps = torch.randn_like(std)\n",
    "        x_sample = eps.mul(std).add_(z_mu)\n",
    "\n",
    "        # decode\n",
    "        predicted = self.dec(x_sample)\n",
    "        return predicted, z_mu, z_var\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encoder\n",
    "encoder = Encoder(INPUT_DIM, HIDDEN_DIM, LATENT_DIM)\n",
    "\n",
    "# decoder\n",
    "decoder = Decoder(LATENT_DIM, HIDDEN_DIM, INPUT_DIM)\n",
    "\n",
    "# vae\n",
    "model = VAE(encoder, decoder).to(device)\n",
    "\n",
    "# optimizer\n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    # set the train mode\n",
    "    model.train()\n",
    "\n",
    "    # loss of the epoch\n",
    "    train_loss = 0\n",
    "\n",
    "    for i, (x, _) in enumerate(train_iterator):\n",
    "        # reshape the data into [batch_size, 784]\n",
    "        x = x.view(-1, 28 * 28)\n",
    "        x = x.to(device)\n",
    "\n",
    "        # update the gradients to zero\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward pass\n",
    "        x_sample, z_mu, z_var = model(x)\n",
    "\n",
    "        # reconstruction loss\n",
    "        recon_loss = F.binary_cross_entropy(x_sample, x, size_average=False)\n",
    "\n",
    "        # kl divergence loss\n",
    "        kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1.0 - z_var)\n",
    "\n",
    "        # total loss\n",
    "        loss = recon_loss + kl_loss\n",
    "\n",
    "        # backward pass\n",
    "        loss.backward()\n",
    "        train_loss += loss.item()\n",
    "\n",
    "        # update the weights\n",
    "        optimizer.step()\n",
    "\n",
    "    return train_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test():\n",
    "    # set the evaluation mode\n",
    "    model.eval()\n",
    "\n",
    "    # test loss for the data\n",
    "    test_loss = 0\n",
    "\n",
    "    # we don't need to track the gradients, since we are not updating the parameters during evaluation / testing\n",
    "    with torch.no_grad():\n",
    "        for i, (x, _) in enumerate(test_iterator):\n",
    "            # reshape the data\n",
    "            x = x.view(-1, 28 * 28)\n",
    "            x = x.to(device)\n",
    "\n",
    "            # forward pass\n",
    "            x_sample, z_mu, z_var = model(x)\n",
    "\n",
    "            # reconstruction loss\n",
    "            recon_loss = F.binary_cross_entropy(x_sample, x, size_average=False)\n",
    "\n",
    "            # kl divergence loss\n",
    "            kl_loss = 0.5 * torch.sum(torch.exp(z_var) + z_mu**2 - 1.0 - z_var)\n",
    "\n",
    "            # total loss\n",
    "            loss = recon_loss + kl_loss\n",
    "            test_loss += loss.item()\n",
    "\n",
    "    return test_loss\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 10%|████████▎                                                                          | 1/10 [00:40<06:06, 40.76s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0, Train Loss: 122.94, Test Loss: 116.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 20%|████████████████▌                                                                  | 2/10 [01:26<05:38, 42.36s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 115.10, Test Loss: 112.89\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 30%|████████████████████████▉                                                          | 3/10 [02:15<05:08, 44.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2, Train Loss: 112.16, Test Loss: 110.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 40%|█████████████████████████████████▏                                                 | 4/10 [03:05<04:36, 46.10s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3, Train Loss: 110.48, Test Loss: 108.95\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 50%|█████████████████████████████████████████▌                                         | 5/10 [03:49<03:47, 45.40s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4, Train Loss: 109.31, Test Loss: 108.28\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 60%|█████████████████████████████████████████████████▊                                 | 6/10 [04:30<02:56, 44.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5, Train Loss: 108.50, Test Loss: 107.73\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 70%|██████████████████████████████████████████████████████████                         | 7/10 [05:09<02:07, 42.57s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6, Train Loss: 107.90, Test Loss: 107.47\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 80%|██████████████████████████████████████████████████████████████████▍                | 8/10 [05:48<01:22, 41.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7, Train Loss: 107.46, Test Loss: 107.11\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      " 90%|██████████████████████████████████████████████████████████████████████████▋        | 9/10 [06:26<00:40, 40.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8, Train Loss: 107.06, Test Loss: 106.55\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 10/10 [07:08<00:00, 42.86s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9, Train Loss: 106.74, Test Loss: 106.00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "best_test_loss = float('inf')\n",
    "\n",
    "for e in tqdm(range(N_EPOCHS)):\n",
    "\n",
    "    train_loss = train()\n",
    "    test_loss = test()\n",
    "\n",
    "    train_loss /= len(train_dataset)\n",
    "    test_loss /= len(test_dataset)\n",
    "\n",
    "    print(f'Epoch {e}, Train Loss: {train_loss:.2f}, Test Loss: {test_loss:.2f}')\n",
    "\n",
    "    if best_test_loss > test_loss:\n",
    "        best_test_loss = test_loss\n",
    "        patience_counter = 1\n",
    "    else:\n",
    "        patience_counter += 1\n",
    "\n",
    "    if patience_counter > 3:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 20])\n",
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x1fcd7bd35c8>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# sample and generate a image\n",
    "z = torch.randn(1, LATENT_DIM).to(device)\n",
    "\n",
    "# run only the decoder\n",
    "reconstructed_img = model.dec(z)\n",
    "img = reconstructed_img.view(28, 28).data\n",
    "\n",
    "print(z.shape)\n",
    "print(img.shape)\n",
    "\n",
    "plt.imshow(img, cmap='gray')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Credit to https://graviraja.github.io/vanillavae/; https://github.com/graviraja/pytorch-sample-codes/ for the code)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incorporating Mutual Information to overcome ELBO shortcomings\n",
    "[Zhao et al.](https://arxiv.org/pdf/1706.02262.pdf) identify that with  finite model capacity, the two goals of our objective, maximising the expected likelihood, $\\mathbb{E}_{q_{\\phi}(\\mathbf{z})}[\\log p_{\\theta}(\\mathbf{x|z})]$, and minimizing the KL divergence, $\\mathcal{D}_{KL}\\left(q_{\\phi}(\\mathbf{z}) || p_{\\theta}(\\mathbf{z})\\right)$, can be conflicting. They show some subtle tradeoffs and **failure modes can emerge from optimizing the ELBO objective**.\n",
    "\n",
    "Within the context of this tutorial, they call the following problem the *information preference* problem. Using complicated decoding distribution, $p_{\\theta}(\\mathbf{x} | \\mathbf{z})$, has been shown to improve sample quality on complex image datasets.\n",
    "\n",
    "However, they show that this approach usually neglects $\\mathbf{z}$ altogether. That is, the mutual information between $\\mathbf{z}$ and $\\mathbf{x}$ becomes vanishingly small. To see the proof, please refer to the paper.\n",
    "\n",
    "To remedy this problem, the authors do two things to our ELBO objective:\n",
    "1. Add a scaling parameter $\\lambda$ to the divergence between $q_{\\phi}(\\mathbf{z})$ and $p(\\mathbf{z})$\n",
    "2. Add a scaled *mutual information maximization* term that **prefers high mutual information between $\\mathbf{x}$ and $\\mathbf{z}$**. This encourages the model emphasise latent code and avoids the information preference problem.\n",
    "\n",
    "Our objective is thus: $$\\mathcal{L}_{\\theta, \\phi}(\\mathbf{x}) = \\mathbb{E}_{q_{\\phi}(\\mathbf{z})}[\\log p_{\\theta}(\\mathbf{x|z})] - \\mathcal{D}_{KL}\\left(q_{\\phi}(\\mathbf{z}) || p(\\mathbf{z})\\right) + \\alpha I_q(\\mathbf{x}, \\mathbf{z})$$\n",
    "\n",
    "where $I_q(\\mathbf{x},\\mathbf{z})$ is the mutual information between $x$ and $z$ under the distribution $q_{\\phi}(\\mathbf{x},\\mathbf{z})$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Information Maximising Visual Question Generation\n",
    "\n",
    "Ok! The above gives us the background knowledge needed to understand this paper. Let's introduce what this paper does. Based on an answer category, the algorithm can generate questions which maximise the likelihood of receiving an expected answer:\n",
    "\n",
    "<img src=\"images/imvqg_example.png\" style=\"width: 800px\" />\n",
    "\n",
    "A key point is that the authors want to move away from generic generation - whether this be the generation of questions with uninformative answers (e.g. \"yes\"), or generic questions (e.g. \"what is this person doing?\"). They present an algorithm which asks questions conditioned on answer categories. This is a deviation from current variational based QG approaches as these models do not guarantee that the question will result in a specific type of answer.\n",
    "\n",
    "They remedy this problem by encoding a desired answer with the image before generating the question (latent space). Obviously there's an issue here - the main purpose of asking questions is to attain an answer; in a real world setting, we don't have access to an answer and therefore don't have anything to encode the image with. Thus, it wouldn't be possible to generate questions without an answer.\n",
    "\n",
    "To enable this question generation, they propose creating a second latent space that is learned from the image and the answer category instead of the answer. At test/inference time, we sample from this second latent space instead of the first. This is feasible because they minimize the KL-divergencebetween the two latent space during training. Not only does this allow us to generate visual questions that maximize mutual information with the expected answer, it also acts as a regularizer into the original latent space. It prevents the learned latent spaces from overfitting to specific answers in the training set and forces them to generalize to categories of questions\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"images/imvqg_approach.png\" style=\"width: 800px\" />\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "They annotate VQA with 15 categories (e.g. objects, attributes, colors, materials, time, etc.) for the top 500 answers.\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "<img src=\"images/imvqg_architecture.png\" style=\"width: 800px\" />\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Let $q$ denote the question we want to generate for an image $i$. This  question  should  result  in  the  an  answer a of category $c$. For example, the question \"What is the person in  red  doing  with  the  ball?\"   should  result  in  the  answer \"kicking\", which belong to category \"activity\". Our final goal is to define a model $p(q|i,c)$. But first, let’s attempt to define a simpler model $p(q|i,a)$ that **maximizes the mutual information between the image and the question $I(i, q)$ and between the expected answer and the question $I(a, q)$**. An objective is thus formulated: $$\\max I(i,q) + \\lambda I(a,q)$$\n",
    "\n",
    "where $q \\sim p(q|i, a)$. $\\lambda$ is a hyperparameter that adjusts for their relative importance during optimization.\n",
    "\n",
    "A latent space, $z$, is introduced, and a mapping between the image and expected answer, $p_{\\theta}(z|i, a)$, is learned. With this $z$ space, our objective is: $$ \\max_{\\theta}I(q, z | a, i) + \\lambda_1 I(a, z) + \\lambda_2 I(i, z) $$ where $z \\sim p_{\\theta}(z|i, a)$ and $q \\sim p_{\\theta}(q|z)$. Again, the $\\lambda$'s weight the respective information terms.\n",
    "\n",
    "Optimizing the mutual information maximisation is intractable because we need knowledge of the posteriors $p(z|i)$ and $p(z|a)$. As a workaround, we can optimize ELBO. Recall that $I(X, Y) = \\mathbb{H}(Y) - \\mathbb{H}(Y|X)$ where $\\mathbb{H}(Y|X) = \\int_{x, y} p(x, y) \\log \\frac{p(x,y)}{p(x)} dxdy$.\n",
    "\n",
    "$$ I(z, i) = \\mathbb{H}(i) - \\mathbb{H}(i | z) \\\\\n",
    "= \\mathbb{H}(i) + \\mathbb{E}_{z \\sim p(z,i)} \\left[ \\mathbb{E}_{\\hat{i} \\sim p(i,z)} \\left[ \\log p(\\hat{i}|z) \\right] \\right] \\\\\n",
    "= \\mathbb{H}(i) + \\mathbb{E}_{i \\sim p(i)} \\left[ \\mathcal{D}_{KL} [p(\\hat{i}|z) || p_{\\theta}(\\hat{i}|z) ] + \\mathbb{E}_{\\hat{i} \\sim p(i|z)} [ \\log p_{\\theta}(\\hat{i} | z) ] \\right] \\\\\n",
    "\\geq \\mathbb{H}(i) + \\mathbb{E}_{i \\sim p(i)} \\left[ \\mathbb{E}_{\\hat{i} \\sim p(i|z)} [ \\log p_{\\theta}(\\hat{i}|z)] \\right]\n",
    "$$\n",
    "\n",
    "Similary, $$ I(z, a) \\geq \\mathbb{H}(a) + \\mathbb{E}_{a \\sim p(a)} \\left[ \\mathbb{E}_{\\hat{a} \\sim p(a|z)} [ \\log p_{\\theta}(\\hat{a}|z)] \\right] $$\n",
    "\n",
    "$I(z, q | a, i)$ can also be bounded: $$I(z, q | a, i) \\geq \\mathbb{H}(q) + \\mathbb{E}_{q \\sim p(q | i,a)} \\left[ \\mathbb{E}_{\\hat{q} \\sim p(q|i, a, z)} [ \\log p_{\\theta}(\\hat{q}|i, a, z)] \\right]$$\n",
    "\n",
    "where $p(q|i, a, z) = p(q|z)p(z|a, i)$.\n",
    "\n",
    "Sticking all these terms into our $z$-space objective (and ignoring any terms that don't involve $\\theta$):\n",
    "$$ \\max_{\\theta} \\mathbb{E}_{p_{\\theta}(q, i, a)} \\left[ \\log p_{\\theta}(q | i, a, z) + \\lambda_1 \\log p_{\\theta}(a|z) + \\lambda_2 \\log p_{\\theta}(i | z) \\right]$$\n",
    "\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Taking a look at the diagram, we see that our $z$ latent space reconstructs the image, $\\hat{h}_i$ and the answer, $\\hat{h}_a$. The second and third term in our objective is optimized by minimizing the $\\mathcal{L_2}$ losses between the original encoding, $h_{\\bullet}$, and the reconstruction, $\\hat{h}_{\\bullet}$ for both the image and answer. The first term is optimized by the MLE between predicted RNN output $\\hat{q}$ and the ground truth $q$.\n",
    "\n",
    "Whew! That's a lot. Let's take stock:\n",
    "-  We've built a model that maximizes the lower bound of mutual information between a latent space, the image and the expected answer\n",
    "- This allows us to generate questions if we know what the expected answer should be\n",
    "- This isn't a feasible set up because in a real-world situation, we do not know the answer\n",
    "\n",
    "To remedy this, a second latent space, $t$, is proposed. This latent space will be based of the fusion of an image and a answer category (instead of an answer). The categories, $h_c$ are encoded as a one-hot vector. $t$ space is trained by minimizing the KL divergence with $z$-space: $$ \\mathcal{L}_t = \\mathcal{D}_{KL}(p_{\\theta}(z|i, a) || p_{\\phi}(t|i,c)) \\\\\n",
    "= \\log \\sigma_t - \\log \\sigma_z + \\frac{\\sigma_z + (\\mu_t - \\mu_z)^2}{2\\sigma_t} - 0.5\n",
    "$$\n",
    "\n",
    "where $\\phi$ are the parameters used to embed $t$-space. This allows us to now utilize $p_{\\phi}(t|i,c)$ to embed into a space that closely resembles $z$-space. Since we assume that both $z$-space and $t$-space follow a multivariate Gaussian with diagonal covariance, the KL term has the analytical form shown above. We no longer need to know the answer to embed and generate questions.\n",
    "\n",
    "The final loss of our model is thus: $$ \\mathcal{L} = \\mathcal{L}_{MLE} + \\lambda_1 \\mathcal{L}_{a} + \\lambda_2 \\mathcal{L}_{i} + \\lambda_3 \\mathcal{L}_{t} $$\n",
    "\n",
    "During inference, we are given an image $i$ and answer category $c$, and are expected to generate questions.  We encode the inputs into $t$-space and sample from it to generate questions, as shown in below.  This allows us to generate goal-driven questions for any image, focused towards extracting its objects, attributes, etc.\n",
    "\n",
    "<img src=\"images/imvqg_inference.png\" style=\"width: 800px\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Code run through?? https://github.com/ranjaykrishna/iq"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
